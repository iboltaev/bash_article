# Article about using Bash in SQL-style for text processing

#bash, #tutorial, #linux, #sql

Приветствую! Данная небольшая статья призвана осветить некоторые аспекты применения Bash для анализа файлов в SQL-стиле. Думаю,
будет интересна для новичков, возмжно, опытные пользователи также найдут для себя что-нибудь новое.

Структура задачи:
- projects/
  - project1/
    - conf/
        # конфигурации построения отчетов по таблицам
      - [run configurations]*.conf  
    - reports/
        # папки с конфигурациями
      - [run configurations]/   
          # сами отчеты, содержат статистику по таблицам Apache Hive
        - report1.json              
        - report2.json
          ...
      .../
  - project2/
    .../
  
Надо: найти просроченные отчеты.

Итак, расчехляем Bash, открываем отдельный терминал для man-ов и приступаем)

Всех, кому интересно - прошу под кат.

----------------------------------------------------------------------------------------------------------------------------------

  Итак, мы имеем: внутреннюю систему построения отчетов в виде папки с проектами. В каждом проекте в папке conf лежат конфигурации
построения отчетов, содержащие в себе имена Hive-овых баз данных в полях "*schema*", по таблицам которых строятся отчеты. В папке 
reports - сами отчеты, разложенные в папки с именами конфигураций. Каждый отчет - это json, содержащий статистику по Hive-овым таблицам 
в массиве объектов "table", а также дату создания в поле "created_date". Возьмем ее вместо даты создания файла, раз уж есть. Нам надо
найти такие отчеты, в которых содержатся таблицы, которые были изменены после создания отчета.

  Почему в SQL-стиле? Bash предоставляет большие возможности работы с текстом, разделенным на колонки (обычно пробелами), напоминающие обработку таблиц в SQL. 
  Наш инструментарий:
  - cat, find, grep и прочее - в представлении не нуждаются)
  - sed - используем для тупой автозамены
  - awk - позволяет отображать/переставлять/сливать колонки, фильтровать строки по содержимому колонок
  - sort, uniq - наверное, любимые инструменты разгребателей логов) Первый - сортирует, второй - удаляет/подсчитывает дубликаты.
    Используются для нахождения всяких top N, типа sort -k <field> | uniq -c | sort -k 1 | head -n <N>
  - xargs - обрабатывает поток строк одной командой. Может развернуть строки в argument-list для заданной команды, а может для каждой       строки эту команду выполнить.
  - join - натуральный SQL-евский INNER JOIN. Сливает 2 сортированных файла по значению одного одинакового поля в один, сначала идет
    общее поле, затем оставшиеся поля первого файла, потом - второго.

  Приступим. Для начала - просто нагрепаем используемые таблицы. Проекты содержат в себе большое количество разного мусора, так что
 запуск в лоб
```  
  grep -r "\"table\":" projects
```  
отрабатывает сильно долго. Поэтому делаем так:
```
  find projects -maxdepth 1 | sed 's/$/\/reports\//g' | xargs -n1 -I dr grep -r "\"table\":" dr | ...
```  
Берем содержимое первого уровня папки projects, добавляем в конец /reports/, и только теперь в каждой такой папке запускаем grep -r.
Он отдает нам данные в таком виде:
```  
projects/project1/reports/run1/<report1>.json:            "table": "table1",
projects/project1/reports/run1/<report1>.json:            "table": "table2",
projects/project1/reports/run2/<report1>.json:            "table": "table3",
```
```
  ... |  awk '{print $1 " " $3}' | sed 's/[\r\n",:]//g' | grep "\s[A-Za-z0-9]" | sort -k 1b,1 | uniq > report_tables
```

Печатаем первую (файл отчета) и третью (имя таблицы) колонки, чистим мусор sed-ом, затем grep-ом, пересортировываем и сохраняем в нашу
первую таблицу - report_tables.
